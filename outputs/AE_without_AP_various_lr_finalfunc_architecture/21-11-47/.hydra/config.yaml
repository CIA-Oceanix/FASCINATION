paths:
  sound: /DATASET/eNATL/eNATL60_BLB002_sound_speed_regrid_0_1000m.nc
  acc_model_path: /homes/o23gauvr/Documents/th√®se/code/FASCINATION/outputs/accoustic_predictor/ap_training/acoustic_pred_training/checkpoints/val_loss=0.02-epoch=951.ckpt
normalization:
  x_min: 1459.0439165829073
  x_max: 1545.8698054910844
trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: false
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  logger:
    _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ${model.arch_shape}
    sub_dir: ${model.final_act_func}_lr_${model.lr}
  min_epochs: 0
  max_epochs: 1000
  callbacks:
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    save_top_k: 1
    filename: '{val_loss:.2f}-{epoch:02d}'
    mode: min
datamodule:
  _target_: src.data.AutoEncoderDatamodule
  input_da:
    _target_: src.utils.load_sound_speed_fields
    path: ${paths.sound}
  domains:
    train:
      time:
        _target_: builtins.slice
        _args_:
        - 0
        - 254
    val:
      time:
        _target_: builtins.slice
        _args_:
        - 254
        - 331
    test:
      time:
        _target_: builtins.slice
        _args_:
        - 331
        - 365
  dl_kw:
    batch_size: 4
    num_workers: 4
  x_min: ${normalization.x_min}
  x_max: ${normalization.x_max}
  pickle_path: ${trainer.logger.save_dir}/pickle/time_idx_per_split.pickle
model:
  _target_: src.autoencoder.AutoEncoder
  x_min: ${normalization.x_min}
  x_max: ${normalization.x_max}
  lr: 0.01
  arch_shape: 4_15_test
  final_act_func: relu
  acoustic_predictor:
    _target_: src.acoustic_predictor.AcousticPredictor.load_from_checkpoint
    checkpoint_path: ${paths.acc_model_path}
    input_depth: 107
  accoustic_training: false
entrypoints:
- _target_: pytorch_lightning.seed_everything
  seed: 333
- _target_: src.train.base_training
  trainer: ${trainer}
  lit_mod: ${model}
  dm: ${datamodule}
