# @package _global_

paths:
    ssh: /DATASET/NATL/NATL60-CJM165_NATL_ssh_y2013.1y.nc
    sst: /DATASET/NATL/NATL60-CJM165_NATL_sst_y2013.1y.nc

trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: False
  accelerator: gpu
  devices: 1
  logger: 
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ''
  min_epochs: 0
  max_epochs: 500
  callbacks:
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: src.early_stopping.ValidEarlyStopping
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_loss
      save_top_k: 1
      filename: '{val_loss:.2f}-{epoch:02d}'
      mode: min

datamodule:
  _target_: src.data.UNetDataModule
  io_time_steps: 2
  input_da: 
    _target_: src.utils.load_data
    path1: ${paths.ssh}
    path2: ${paths.sst}
  domains:
    train:
      time: {_target_: builtins.slice, _args_: ['2013-01-01', '2013-09-12']}
    val: 
      time: {_target_: builtins.slice, _args_: ['2013-09-13', '2013-11-24']}
    test: 
      time: {_target_: builtins.slice,  _args_: ['2013-11-25', '2013-12-30']}
  dl_kw: {batch_size: 4, num_workers: 1}

model:
  _target_: src.unet.UNet
  n_var: 2
  io_time_steps: 2
  integration_steps: 2
  loss_by_step: 1


entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333
  - _target_: src.train.base_training
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}
      # _target_: src.utils.diagnostics
      # _partial_: true
      # test_domain:
      #   time: {_target_: builtins.slice, _args_: ["2012-10-22", "2012-12-02"]}
      #   lat: {_target_: builtins.slice, _args_: [33, 43]}
      #   lon: {_target_: builtins.slice, _args_: [-65, -55]}
